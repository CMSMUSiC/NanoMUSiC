#!/usr/bin/env python3

from enum import Enum
from typing import Union
from distribution_model import DistributionType
from matplotlib import is_interactive
from typing_extensions import Annotated
import typer
import metadata
import sys
import os
import time
import subprocess
from functools import wraps


def execution_time(func):
    @wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.time()
        result = func(*args, **kwargs)
        end_time = time.time()
        elapsed_time = end_time - start_time
        formatted_time = time.strftime("%H:%M:%S", time.gmtime(elapsed_time))
        print(f"Execution time of {func.__name__}: {formatted_time}")
        return result

    return wrapper


############################################################################
############################################################################
# Classification
############################################################################
############################################################################
classification_app = typer.Typer(
    help="Launch and control the Classification steps. Expected workflow: build -> preprocess -> launch -> merge -> unwrap -> fold -> make-distributions.",
    pretty_exceptions_show_locals=False,
)


@classification_app.command()
@execution_time
def parse(
    samples_list: Annotated[
        str, typer.Argument(help="File with DAS names of all samples (one per line).")
    ] = "samples.txt",
):
    """
    Will parse the list of DAS names and format it in the MUSiC format, including x-sections.
    """
    import parse_sample_list

    parse_sample_list.parse(samples_list)


@classification_app.command()
@execution_time
def build(
    config_file: Annotated[
        str,
        typer.Argument(
            help="parsed_sample_list.toml file path, as build by the parser."
        ),
    ] = "parsed_sample_list.toml",
):
    """
    Will query all files LFN, for each sample and add it to the analysis_config.toml.
    """
    import analysis_builder

    analysis_builder.build_analysis_config(config_file)


@classification_app.command()
@execution_time
def preprocess(
    config_file: Annotated[
        str,
        typer.Argument(help="analysis_config.toml file path, as provided by builder."),
    ],
):
    """
    Will scan all MC input files and collect the total MC weight per sample.
    """
    import preprocessing

    preprocessing.preamble()
    preprocessing.compute_sum_weights(config_file)


class ClassificationTarget(str, Enum):
    Condor = "condor"
    Dev = "dev"
    Parallel = "parallel"


@classification_app.command()
@execution_time
def launch(
    config_file: Annotated[
        str,
        typer.Argument(help="analysis_config.toml file path"),
    ],
    description: Annotated[
        Union[str, None],
        typer.Argument(
            help="description file path of the classification being launched"
        ),
    ] = None,
    target: ClassificationTarget = ClassificationTarget.Parallel,
    process: Union[str, None] = None,
    year: Union[metadata.Years, None] = None,
    max_files: int = sys.maxsize,
    split_size: int = sys.maxsize,
    dry_run: bool = False,
    num_cpus: int = 128,
):
    """
    Launch the classification jobs for a given target.
    """

    # Read the file content and remove whitespace characters
    def normalize_content(file_path):
        with open(file_path, "r") as file:
            content = file.read()
            normalized_content = "".join(content.split())
        return normalized_content

    if not description:
        description_template_file = (
            "{}/NanoMUSiC/Classification/assets/description_template.txt".format(
                os.getenv("MUSIC_BASE")
            )
        )

        os.system("cp {} description.txt".format(description_template_file))
        os.system("/usr/bin/vim +normal\\ G description.txt")

        if normalize_content(description_template_file) == normalize_content(
            "description.txt"
        ):
            print(
                "ERROR: Could not set description. Description should not be empty.",
                file=sys.stderr,
            )
            sys.exit(-1)
    else:
        if description.endswith(".txt"):
            if os.path.isfile(description):
                proc = subprocess.run(
                    "cp {} description.txt".format(description),
                    shell=True,
                    capture_output=True,
                    text=True,
                )
                if proc.returncode != 0:
                    print(
                        "ERROR: Could not copy description file.\n{}\n{}".format(
                            proc.stdout, proc.stderr
                        ),
                        file=sys.stderr,
                    )
                    sys.exit(-1)
            else:
                print(
                    "ERROR: Could not set description. File ({}) not found.".format(
                        description
                    ),
                    file=sys.stderr,
                )
                sys.exit(-1)

        else:
            print(
                "ERROR: Could not copy description file. Description file should end with .txt",
                file=sys.stderr,
            )
            sys.exit(-1)

    print("Creating source code tarball ...")
    os.system("rm -rf classification_src.tar.gz ")
    proc = subprocess.run(
        "tar --exclude-from=$MUSIC_BASE/.gitignore --exclude=build --exclude=.git --exclude=.ruff_cache --exclude=.rucio_seth --exclude=.rucio_silva --exclude=Legacy --exclude=opt --exclude=bin --exclude=lib -czvf classification_src.tar.gz -C $MUSIC_BASE .",
        shell=True,
        capture_output=True,
        text=True,
    )
    if proc.returncode != 0:
        print(
            "ERROR: Could create source code tarball.\n{}\n{}".format(
                proc.stdout, proc.stderr
            ),
            file=sys.stderr,
        )
        sys.exit(-1)

    import classification

    if not os.path.exists("sum_weights.json"):
        print("ERROR: Could not find sum_weights.json. Did you run the preprocess?")
        sys.exit(-1)

    if target == ClassificationTarget.Dev:
        classification.launch_dev(config_file, process, year, max_files)
    elif target == ClassificationTarget.Parallel:
        classification.launch_parallel(
            config_file, process, year, max_files, 1, num_cpus
        )
    else:
        classification.launch_condor(
            config_file, process, year, max_files, split_size, dry_run
        )


@classification_app.command()
@execution_time
def merge(
    config_file: Annotated[
        str,
        typer.Argument(help="analysis_config.toml file path"),
    ],
    inputs_dir: str = "classification_outputs",
):
    """
    Merge classification results.
    """
    import classification

    classification.merge_classification_outputs(config_file, inputs_dir)


@classification_app.command()
@execution_time
def unwrap(
    config_file: Annotated[
        str,
        typer.Argument(help="analysis_config.toml file path"),
    ],
    inputs_dir: str = "classification_merged_results",
    validation_inputs_dir: str = "validation_merged_results",
    num_cpus: int = 128,
):
    """
    Will serialize to ROOT (TH1F) all recorded EventClasses.
    """
    import classification

    classification.serialize_to_root(
        config_file, inputs_dir, validation_inputs_dir, num_cpus
    )


@classification_app.command()
@execution_time
def fold(
    inputs_dir: str = "classification_root_files",
    validation_inputs_dir: str = "validation_root_files",
):
    """
    Will merge related histograms (TH1F) into one ROOT file per event class.
    """
    import classification

    classification.fold(
        inputs_dir,
        validation_inputs_dir,
    )


class FoldAndPlotTarget(str, Enum):
    Classification = "classification"
    Validation = "validation"
    All = "all"


@classification_app.command()
@execution_time
def make_distributions(
    config_file: Annotated[
        str,
        typer.Argument(help="analysis_config.toml file path"),
    ]
    | None = None,
    alternative_config_file: Annotated[
        str,
        typer.Argument(
            help="Altrernative analysis_config.toml file path, with different cross-sections"
        ),
    ]
    | None = None,
    inputs_dir: str = "classification_folded_files",
    validation_inputs_dir: str = "validation_folded_files",
    name_filter: str = "*",
    target: FoldAndPlotTarget = FoldAndPlotTarget.All,
):
    """
    Will merge related histograms (TH1F) into Distributions (usefull data format for plotting and scan).
    """
    import classification

    if target == FoldAndPlotTarget.Classification or target == FoldAndPlotTarget.All:
        classification.make_distributions(
            config_file,
            alternative_config_file,
            inputs_dir,
            validation_inputs_dir,
            name_filter,
            None,
        )
    if target == FoldAndPlotTarget.Validation or target == FoldAndPlotTarget.All:
        classification.make_distributions(
            config_file,
            alternative_config_file,
            inputs_dir,
            validation_inputs_dir,
            None,
            "*" if target == FoldAndPlotTarget.All else name_filter,
        )


############################################################################
############################################################################
# Plotter
############################################################################
############################################################################
plot_app = typer.Typer(
    help="Produce different MUSiC plots.", pretty_exceptions_show_locals=False
)


@plot_app.command()
@execution_time
def distribution(
    input_dir: str = "classification_distributions",
    output_dir: str = "classification_plots",
    filters: list[str] = ["*"],
    num_cpus: int = 128,
):
    """
    Produce plots all distribution, matching (glob matching) the different filters.
    """
    from plotter import plotter

    plotter(input_dir, filters, output_dir, num_cpus)


@plot_app.command()
@execution_time
def validation(
    input_dir: str = "validation_distributions",
    output_dir: str = "validation_plots",
    filters: list[str] = ["*"],
    num_cpus: int = 128,
):
    """
    Produce plots all validation, matching (glob matching) the different filters.
    """
    from plotter import plotter

    plotter(input_dir, filters, output_dir, num_cpus, True)


@plot_app.command()
@execution_time
def most_occupied(
    input_file: str = "classification_plots/integral_pvalues_data.json",
    output_dir: str = "classification_most_occupied",
):
    """
    Plot the occupancy and p_integral of most occupied classes.
    """
    from integral_pvalues import integral_pvalues_summary

    os.system("rm -rf {}".format(output_dir))
    os.system("mkdir -p {}".format(output_dir))

    for y in ["2016", "2017", "2018", "Run2"]:
        integral_pvalues_summary(
            input_file,
            output_dir,
            year=y,
            num_classes=20,
            plot_all=True,
            plot_per_objects=True,
            plot_exclusive=True,
        )
        integral_pvalues_summary(
            input_file,
            output_dir,
            year=y,
            num_classes=40,
            plot_all=True,
            plot_per_objects=True,
            plot_exclusive=True,
        )
        integral_pvalues_summary(
            input_file,
            output_dir,
            year=y,
            num_classes=100,
            plot_all=True,
            plot_per_objects=True,
            plot_exclusive=True,
        )


@plot_app.command()
@execution_time
def most_discrepant(
    input_file: str = "classification_plots/integral_pvalues_data.json",
    output_dir: str = "classification_most_discrepant",
):
    """
    Plot the occupancy and p_integral of most discrepant classes.
    """
    from integral_pvalues import integral_pvalues_summary, IntegralPValuePlotType

    os.system("rm -rf {}".format(output_dir))
    os.system("mkdir -p {}".format(output_dir))

    for y in ["2016", "2017", "2018", "Run2"]:
        integral_pvalues_summary(
            input_file,
            output_dir,
            year=y,
            num_classes=20,
            plot_all=True,
            plot_per_objects=True,
            plot_exclusive=True,
            plot_type=IntegralPValuePlotType.MostDiscrepant,
        )
        integral_pvalues_summary(
            input_file,
            output_dir,
            year=y,
            num_classes=40,
            plot_all=True,
            plot_per_objects=True,
            plot_exclusive=True,
            plot_type=IntegralPValuePlotType.MostDiscrepant,
        )
        integral_pvalues_summary(
            input_file,
            output_dir,
            year=y,
            num_classes=100,
            plot_all=True,
            plot_per_objects=True,
            plot_exclusive=True,
            plot_type=IntegralPValuePlotType.MostDiscrepant,
        )


@plot_app.command()
@execution_time
def summary(input_dir: str = "scan_results", output_dir: str = "scan_summary_plots"):
    from p_tilde import plot_ptilde
    from scan_results import ScanResults

    def is_met(ec_name: str) -> bool:
        return "MET" in ec_name

    def is_sum_pt(ec_name: str) -> bool:
        return "sum_pt" in ec_name

    def is_invariant_mass(ec_name: str) -> bool:
        return "invariant_mass" in ec_name

    n_rounds, p_tilde_props = make_p_tilde_props(filters=[is_met])
    plot_ptilde(p_tilde_props, n_rounds, output_dir, "all_met", "All classes")

    n_rounds, p_tilde_props = make_p_tilde_props(filters=[is_sum_pt])
    plot_ptilde(p_tilde_props, n_rounds, output_dir, "all_sum_pt", "All classes")

    n_rounds, p_tilde_props = make_p_tilde_props(filters=[is_invariant_mass])
    plot_ptilde(
        p_tilde_props, n_rounds, output_dir, "all_invariant_mass", "All classes"
    )

    n_rounds, p_tilde_props = make_p_tilde_props(filters=[is_met])
    plot_ptilde(
        p_tilde_props, n_rounds, output_dir, "exclusive_met", "All exclusive classes"
    )

    n_rounds, p_tilde_props = make_p_tilde_props(filters=[is_sum_pt])
    plot_ptilde(
        p_tilde_props, n_rounds, output_dir, "exclusive_sum_pt", "All exclusive classes"
    )

    n_rounds, p_tilde_props = make_p_tilde_props(filters=[is_invariant_mass])
    plot_ptilde(
        p_tilde_props,
        n_rounds,
        output_dir,
        "exclusive_invariant_mass",
        "All exclusive classes",
    )

    n_rounds, p_tilde_props = make_p_tilde_props(filters=[is_met])
    plot_ptilde(
        p_tilde_props, n_rounds, output_dir, "inclusive_met", "All inclusive classes"
    )

    n_rounds, p_tilde_props = make_p_tilde_props(filters=[is_sum_pt])
    plot_ptilde(
        p_tilde_props, n_rounds, output_dir, "inclusive_sum_pt", "All inclusive classes"
    )

    n_rounds, p_tilde_props = make_p_tilde_props(filters=[is_invariant_mass])
    plot_ptilde(
        p_tilde_props,
        n_rounds,
        output_dir,
        "inclusive_invariant_mass",
        "All inclusive classes",
    )

    n_rounds, p_tilde_props = make_p_tilde_props(filters=[is_met])
    plot_ptilde(
        p_tilde_props,
        n_rounds,
        output_dir,
        "jet_inclusive_met",
        "All jet inclusive classes",
    )

    n_rounds, p_tilde_props = make_p_tilde_props(filters=[is_sum_pt])
    plot_ptilde(
        p_tilde_props,
        n_rounds,
        output_dir,
        "jet_inclusive_sum_pt",
        "All jet inclusive classes",
    )

    n_rounds, p_tilde_props = make_p_tilde_props(filters=[is_invariant_mass])
    plot_ptilde(
        p_tilde_props,
        n_rounds,
        output_dir,
        "jet_inclusive_invariant_mass",
        "All jet inclusive classes",
    )


@plot_app.command()
@execution_time
def web_summary():
    """
    Make web summary of file contents.
    """
    from web_summary import make_web_summary

    make_web_summary("./classification_most_occupied")
    make_web_summary("./classification_most_discrepant")
    make_web_summary("./classification_plots")
    make_web_summary("./validation_plots")
    make_web_summary("./scan_summary_plots")


############################################################################
############################################################################
# Scanner
############################################################################
############################################################################
scan_app = typer.Typer(
    help="Start and monitor Region of Interest scans.",
    pretty_exceptions_show_locals=False,
)


class DistributionsType(str, Enum):
    InvariantMass = "invariant_mass"
    SumPt = "sum_pt"
    MET = "met"


@scan_app.command()
@execution_time
def start(
    distribution: Union[DistributionsType, None] = None,
    input_dir: str = "classification_distributions",
    filters: list[str] = ["*"],
    output_dir: str = "scan_results",
    num_cpus: int = 128,
    do_clean: bool = False,
    n_rounds: int = 100_000,
    split_size: int = 1_000,
):
    """
    Launch RoIScan.
    """
    if split_size < 1:
        print("ERROR: Could not start scan. split_size should be > 0.", file=sys.stderr)
        sys.exit(1)

    if n_rounds < 1:
        print("ERROR: Could not start scan. n_rounds should be > 0.", file=sys.stderr)
        sys.exit(1)

    from scan import launch_scan

    if distribution:
        launch_scan(
            input_dir,
            filters,
            distribution.value,
            output_dir,
            num_cpus,
            do_clean,
            n_rounds,
            split_size,
        )
    else:
        launch_scan(
            input_dir,
            filters,
            DistributionsType.InvariantMass.value,
            output_dir,
            num_cpus,
            do_clean,
            n_rounds,
            split_size,
        )
        launch_scan(
            input_dir,
            filters,
            DistributionsType.SumPt.value,
            output_dir,
            num_cpus,
            do_clean,
            n_rounds,
            split_size,
        )
        launch_scan(
            input_dir,
            filters,
            DistributionsType.MET.value,
            output_dir,
            num_cpus,
            do_clean,
            n_rounds,
            split_size,
        )


@scan_app.command()
@execution_time
def get_p_tilde(
    class_name: str,
    distribution: str,
    input_dir: str = "scan_results",
):
    """
    Calculate p-tilde for a given class and distribution.
    """
    from scan import get_p_tilde

    get_p_tilde(
        "{}/{}/{}_{}_data_0_info.json".format(
            input_dir,
            class_name.replace("+", "_"),
            class_name.replace("+", "_"),
            distribution,
        ),
        "{}/{}/{}_{}_mc_*_info.json".format(
            input_dir,
            class_name.replace("+", "_"),
            class_name.replace("+", "_"),
            distribution,
        ),
    )


############################################################################
############################################################################
# Main
############################################################################
############################################################################
app = typer.Typer(pretty_exceptions_show_locals=False)
app.add_typer(classification_app, name="classification")
app.add_typer(plot_app, name="plot")
app.add_typer(scan_app, name="scan")

if __name__ == "__main__":
    app()
